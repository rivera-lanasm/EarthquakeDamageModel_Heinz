{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import fiona\n",
    "import pyogrio\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if directory exist if not make it, return path\n",
    "def make_data_path():\n",
    "    \"\"\"Create directories for data storage if they do not exist.\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    parent = os.path.dirname(cwd)\n",
    "    data_path = os.path.join(parent, 'Data')\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    building_data_csv = os.path.join(data_path, 'building_data_csv')\n",
    "    building_data_gdb = os.path.join(data_path, 'building_data_gdb')\n",
    "    building_stock_data = os.path.join(data_path, 'building_stock_data')\n",
    "    if not os.path.exists(building_data_csv):\n",
    "        os.makedirs(building_data_csv)\n",
    "    if not os.path.exists(building_data_gdb):\n",
    "        os.makedirs(building_data_gdb)\n",
    "    if not os.path.exists(building_stock_data):\n",
    "        os.makedirs(building_stock_data)\n",
    "\n",
    "    return building_data_csv, building_data_gdb, building_stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD BUILDING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to download building data, extract it, then aggregate the data to count the number of building for each census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_state_links():\n",
    "    \"\"\"Fetches state names and their corresponding links from the webpage.\"\"\"\n",
    "   \n",
    "    # URL of the webpage to scrape\n",
    "    url = \"https://disasters.geoplatform.gov/USA_Structures/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        return {link.text.strip(): link[\"href\"] for link in links if \"Deliverable\" in link[\"href\"]}\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return {}\n",
    "\n",
    "def get_link_by_state(state_name, state_links):\n",
    "    \"\"\"Returns the link for a given state name.\"\"\"\n",
    "    return state_links.get(state_name, \"State not found\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(state_name, state_links):\n",
    "    \"\"\"Downloads and extracts a ZIP file from the given URL.\n",
    "    Keyword arguments:\n",
    "    state_name -- Name of the state\n",
    "    state_links -- Corresponding links for each state\n",
    "    \"\"\"\n",
    "    url = get_link_by_state(state_name, state_links)\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    output_dir = os.path.join(parent_dir, 'Data', 'building_data_gdb')\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        zip_path = os.path.join(output_dir, f\"{state_name}_Structures.zip\")\n",
    "        \n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(f\"Downloaded, extracted, and deleted ZIP file for {state_name} to {output_dir}\")\n",
    "    else:\n",
    "        print(\"Failed to download the ZIP file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alabama': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Alabama/Deliverable202496AL.zip',\n",
       " 'Alaska': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Alaska/Deliverable20230728AK.zip',\n",
       " 'American Samoa': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/American+Samoa/Deliverable20230831AS.zip',\n",
       " 'Arizona': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Arizona/Deliverable20230502AZ.zip',\n",
       " 'Arkansas': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Arkansas/Deliverable20230630AR.zip',\n",
       " 'California': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/California/Deliverable20230728CA.zip',\n",
       " 'Colorado': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Colorado/Deliverable20230630CO.zip',\n",
       " 'Connecticut': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Connecticut/Deliverable20230502CT.zip',\n",
       " 'Delaware': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Delaware/Deliverable20230630DE.zip',\n",
       " 'D.C.': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/District+of+Columbia/Deliverable20230502DC.zip',\n",
       " 'Guam': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Guam/Deliverable20230831GU.zip',\n",
       " 'Florida': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Florida/Deliverable202496FL.zip',\n",
       " 'Georgia': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Georgia/Deliverable202496GA.zip',\n",
       " 'Hawaii': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Hawaii/Deliverable20230526HI.zip',\n",
       " 'Idaho': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Idaho/Deliverable20230526ID.zip',\n",
       " 'Illinois': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Illinois/Deliverable20230831IL.zip',\n",
       " 'Indiana': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Indiana/Deliverable20230502IN.zip',\n",
       " 'Iowa': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Iowa/Deliverable20230502IA.zip',\n",
       " 'Kansas': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Kansas/Deliverable20230630KS.zip',\n",
       " 'Kentucky': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Kentucky/Deliverable20230728KY.zip',\n",
       " 'Louisiana': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Louisiana/Deliverable202496LA.zip',\n",
       " 'Maine': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Maine/Deliverable20230502ME.zip',\n",
       " 'Maryland': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Maryland/Deliverable20230728MD.zip',\n",
       " 'Massachusetts': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Massachusetts/Deliverable20230502MA.zip',\n",
       " 'Michigan': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Michigan/Deliverable20230526MI.zip',\n",
       " 'Minnesota': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Minnesota/Deliverable20230728MN.zip',\n",
       " 'Missouri': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Missouri/Deliverable20230728MO.zip',\n",
       " 'Mississippi': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Mississippi/Deliverable202496MS.zip',\n",
       " 'Montana': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Montana/Deliverable20230502MT.zip',\n",
       " 'Nebraska': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Nebraska/Deliverable20230831NE.zip',\n",
       " 'Nevada': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Nevada/Deliverable20230526NV.zip',\n",
       " 'New Hampshire': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+Hampshire/Deliverable20230526NH.zip',\n",
       " 'New Jersey': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+Jersey/Deliverable20230502NJ.zip',\n",
       " 'New Mexico': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+Mexico/Deliverable20230502NM.zip',\n",
       " 'New York': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+York/Deliverable20230502NY.zip',\n",
       " 'North Carolina': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/North+Carolina/Deliverable202496NC.zip',\n",
       " 'North Dakota': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/North+Dakota/Deliverable20230728ND.zip',\n",
       " 'Northern Mariana Islands': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Northern+Mariana+Islands/Deliverable20230831MP.zip',\n",
       " 'Ohio': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Ohio/Deliverable20230502OH.zip',\n",
       " 'Oklahoma': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Oklahoma/Deliverable20231003OK.zip',\n",
       " 'Oregon': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Oregon/Deliverable20230526OR.zip',\n",
       " 'Pennsylvania': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Pennsylvania/Deliverable20230831PA.zip',\n",
       " 'Puerto Rico': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Puerto+Rico/Deliverable20230630PR.zip',\n",
       " 'Rhode Island': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Rhode+Island/Deliverable20230502RI.zip',\n",
       " 'South Carolina': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/South+Carolina/Deliverable202496SC.zip',\n",
       " 'South Dakota': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/South+Dakota/Deliverable20230728SD.zip',\n",
       " 'Tennessee': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Tennessee/Deliverable20230502TN.zip',\n",
       " 'Texas': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Texas/Deliverable202496TX.zip',\n",
       " 'Utah': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Utah/Deliverable20230502UT.zip',\n",
       " 'Vermont': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Vermont/Deliverable20230502VT.zip',\n",
       " 'Virgin Islands': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Virgin+Islands/Deliverable20230630VI.zip',\n",
       " 'Virginia': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Virginia/Deliverable202496VA.zip',\n",
       " 'Washington': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Washington/Deliverable20230831WA.zip',\n",
       " 'West Virginia': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/West+Virginia/Deliverable20230831WV.zip',\n",
       " 'Wisconsin': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Wisconsin/Deliverable20230728WI.zip',\n",
       " 'Wyoming': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Wyoming/Deliverable20230728WY.zip'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_links = fetch_state_links()\n",
    "state_links\n",
    "# download_and_extract_zip(\"Alaska\", state_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded, extracted, and deleted ZIP file for Alabama to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Alaska to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for American Samoa to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Arizona to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Arkansas to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for California to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Colorado to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Connecticut to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Delaware to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for D.C. to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Guam to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Florida to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Georgia to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Hawaii to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Idaho to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Illinois to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Indiana to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Iowa to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Kansas to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Kentucky to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Louisiana to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Maine to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Maryland to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Massachusetts to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Michigan to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Minnesota to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Missouri to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Mississippi to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Montana to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Nebraska to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Nevada to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New Hampshire to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New Jersey to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New Mexico to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New York to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for North Carolina to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for North Dakota to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Northern Mariana Islands to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Ohio to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Oklahoma to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Oregon to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Pennsylvania to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Puerto Rico to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Rhode Island to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for South Carolina to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for South Dakota to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Tennessee to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Texas to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Utah to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Vermont to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Virgin Islands to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Virginia to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Washington to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for West Virginia to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Wisconsin to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Wyoming to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n"
     ]
    }
   ],
   "source": [
    "for state in state_links:\n",
    "    download_and_extract_zip(state, state_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdb_path_by_state(stateid):\n",
    "    \"\"\"Returns the path to the GDB file for a given state ID.\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # get parent directory\n",
    "    parent_dir = os.path.dirname(cwd)\n",
    "    # get the building data directory\n",
    "    building_data_directory = os.path.join(parent_dir, 'Data', 'building_data_gdb')\n",
    "    # find all folder in the building data directory\n",
    "    folders = [f for f in os.listdir(building_data_directory) if os.path.isdir(os.path.join(building_data_directory, f))]\n",
    "    # get the folder that ends with stateid\n",
    "    stateid_dir= [f for f in folders if f.endswith(f'{stateid}')][0]\n",
    "\n",
    "    return os.path.join(building_data_directory, stateid_dir, f'{stateid}_Structures.gdb')\n",
    "\n",
    "def get_building_data_csv(stateid):\n",
    "    \"\"\"Returns the path to the CSV file for a given state ID.\"\"\"\n",
    "    building_data_directory = os.path.join(os.path.dirname(os.getcwd()), 'Data', 'building_data_csv')\n",
    "\n",
    "    # get the csv file\n",
    "    return os.path.join(building_data_directory, f'{stateid}_building_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read only the specified columns\n",
    "def read_cols(path):\n",
    "    \"\"\"Read the GDB file and return a GeoDataFrame with specified columns.\"\"\"\n",
    "    \n",
    "    cols = ['BUILD_ID', 'OCC_CLS', 'PRIM_OCC', 'CENSUSCODE', 'LONGITUDE', 'LATITUDE']\n",
    "    return gpd.read_file(path, columns=cols)\n",
    "    \n",
    "# only read specific columns, it can reduce the memory usage and time for each state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_building_data(stateid):\n",
    "    \"\"\"Check if the aggregated csv file exists for the given state ID.\n",
    "    If it does, return the file type and None. Do nothing.\n",
    "    If it does not, read the GDB file for the state and return the file type and the GeoDataFrame.\n",
    "    Kwargs:\n",
    "    stateid -- State ID\n",
    "    \"\"\"\n",
    "    \n",
    "    # gdb file path\n",
    "    building_data_directory = gdb_path_by_state(stateid)\n",
    "\n",
    "    # get the csv file\n",
    "    csv_path = get_building_data_csv(stateid)\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Aggregated csv file found for {stateid}\")\n",
    "        return 'csv', None\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading {building_data_directory}\")\n",
    "        return 'gdb', read_cols(building_data_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_data = [\n",
    "    (\"Alabama\", \"AL\"), (\"Alaska\", \"AK\"), (\"Arizona\", \"AZ\"), (\"Arkansas\", \"AR\"),\n",
    "    (\"California\", \"CA\"), (\"Colorado\", \"CO\"), (\"Connecticut\", \"CT\"), (\"Delaware\", \"DE\"),\n",
    "    (\"Florida\", \"FL\"), (\"Georgia\", \"GA\"), (\"Hawaii\", \"HI\"), (\"Idaho\", \"ID\"),\n",
    "    (\"Illinois\", \"IL\"), (\"Indiana\", \"IN\"), (\"Iowa\", \"IA\"), (\"Kansas\", \"KS\"),\n",
    "    (\"Kentucky\", \"KY\"), (\"Louisiana\", \"LA\"), (\"Maine\", \"ME\"), (\"Maryland\", \"MD\"),\n",
    "    (\"Massachusetts\", \"MA\"), (\"Michigan\", \"MI\"), (\"Minnesota\", \"MN\"), (\"Mississippi\", \"MS\"),\n",
    "    (\"Missouri\", \"MO\"), (\"Montana\", \"MT\"), (\"Nebraska\", \"NE\"), (\"Nevada\", \"NV\"),\n",
    "    (\"New Hampshire\", \"NH\"), (\"New Jersey\", \"NJ\"), (\"New Mexico\", \"NM\"), (\"New York\", \"NY\"),\n",
    "    (\"North Carolina\", \"NC\"), (\"North Dakota\", \"ND\"), (\"Ohio\", \"OH\"), (\"Oklahoma\", \"OK\"),\n",
    "    (\"Oregon\", \"OR\"), (\"Pennsylvania\", \"PA\"), (\"Rhode Island\", \"RI\"), (\"South Carolina\", \"SC\"),\n",
    "    (\"South Dakota\", \"SD\"), (\"Tennessee\", \"TN\"), (\"Texas\", \"TX\"), (\"Utah\", \"UT\"),\n",
    "    (\"Vermont\", \"VT\"), (\"Virginia\", \"VA\"), (\"Washington\", \"WA\"), (\"West Virginia\", \"WV\"),\n",
    "    (\"Wisconsin\", \"WI\"), (\"Wyoming\", \"WY\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATE BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remap OCC_CLS and PRIM_OCC\n",
    "def remap_occupancy_classes(gdf):\n",
    "    \"\"\"Remap the occupancy classes and primary occupancy from GDB files.\n",
    "    Kwargs:\n",
    "    gdf -- GeoDataFrame read from the GDB file\"\"\"\n",
    "\n",
    "    # Define the mapping dictionaries\n",
    "    building_data = gdf[['BUILD_ID', 'OCC_CLS', 'PRIM_OCC', 'CENSUSCODE', 'LONGITUDE', 'LATITUDE']]\n",
    "    # mapping the occupancy class\n",
    "    mapping = {\n",
    "        'Agriculture':'OTHER', 'Education':'OTHER', 'Residential':'RESIDENTIAL', 'Unclassified':'OTHER',\n",
    "        'Commercial':'OTHER', 'Government':'OTHER', 'Industrial':'OTHER', 'Utility and Misc':'OTHER',\n",
    "        'Assembly':'OTHER'\n",
    "    }\n",
    "    building_data['OCC_CLS'] = building_data['OCC_CLS'].map(mapping)\n",
    "\n",
    "    # mapping the primary occupancy\n",
    "    mapping = {i:'OTHER' for i in building_data['PRIM_OCC'].unique() if i not in ['Single Family Dwelling', 'Multi - Family Dwelling']}\n",
    "    residential = {'Single Family Dwelling':'SINGLE FAMILY', 'Multi - Family Dwelling':'MULTI FAMILY'}\n",
    "    mapping.update(residential)\n",
    "    building_data['PRIM_OCC'] = building_data['PRIM_OCC'].map(mapping)\n",
    "    return building_data\n",
    "\n",
    "\n",
    "# function to aggregate the building counts by GEODI, OCC_CLS, PRIM_OCC\n",
    "def aggregate_building_counts(gdf):\n",
    "    building_data = remap_occupancy_classes(gdf)\n",
    "    # group by GEODI, OCC_CLS, PRIM_OCC and sum the counts\n",
    "    count_building_data = building_data.groupby(['CENSUSCODE', 'OCC_CLS', 'PRIM_OCC']).agg({'BUILD_ID':'count'}).reset_index()\n",
    "    # rename the columns\n",
    "    count_building_data = count_building_data.rename(columns={'BUILD_ID':'COUNT'})\n",
    "    return count_building_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_building_data(count_building_data):\n",
    "    \"\"\"Pivot the building data to get the count of buildings by OCC_CLS and PRIM_OCC.\n",
    "    Kwargs:\n",
    "    count_building_data -- DataFrame with building counts aggregated by CENSUS CODE, OCC_CLS and PRIM_OCC\n",
    "    \"\"\"\n",
    "\n",
    "    df = count_building_data.copy()\n",
    "\n",
    "    # Create a pivot table\n",
    "    df_pivot = df.pivot_table(index=\"CENSUSCODE\", columns=[\"OCC_CLS\", \"PRIM_OCC\"], values=\"COUNT\", aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "    # Flatten MultiIndex columns\n",
    "    df_pivot.columns = [f\"{col[0]}_{col[1]}\" for col in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    df_pivot['TOTAL_RESIDENTIAL'] = df_pivot['RESIDENTIAL_MULTI FAMILY'] + df_pivot['RESIDENTIAL_SINGLE FAMILY'] + df_pivot['RESIDENTIAL_OTHER']\n",
    "    df_pivot['TOTAL_BUILDING'] = df_pivot['TOTAL_RESIDENTIAL'] + df_pivot['OTHER_OTHER']\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading building data for Alaska\n",
      "Reading /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_gdb/Deliverable202496AL/AL_Structures.gdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_31767/1288599890.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['OCC_CLS'] = building_data['OCC_CLS'].map(mapping)\n",
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_31767/1288599890.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['PRIM_OCC'] = building_data['PRIM_OCC'].map(mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved building data for Alaska to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_csv/AL_building_data.csv\n"
     ]
    }
   ],
   "source": [
    "state_name, stateid = (\"Alaska\", \"AL\")\n",
    "print(f\"Reading building data for {state_name}\")\n",
    "filetype, gdf = read_building_data(stateid)\n",
    "if filetype == 'csv':\n",
    "    pass\n",
    "elif filetype == 'gdb':\n",
    "    count_building_data = aggregate_building_counts(gdf)\n",
    "    df_pivot = pivot_building_data(count_building_data)\n",
    "    output_path = os.path.join(os.path.dirname(os.getcwd()), 'Data', 'building_data_csv', f\"{stateid}_building_data.csv\")\n",
    "    df_pivot.to_csv(output_path, index=False)\n",
    "    print(f\"Saved building data for {state_name} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CENSUSCODE</th>\n",
       "      <th>OTHER_OTHER</th>\n",
       "      <th>RESIDENTIAL_MULTI FAMILY</th>\n",
       "      <th>RESIDENTIAL_OTHER</th>\n",
       "      <th>RESIDENTIAL_SINGLE FAMILY</th>\n",
       "      <th>TOTAL_RESIDENTIAL</th>\n",
       "      <th>TOTAL_BUILDING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06001400100</td>\n",
       "      <td>104</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>1162</td>\n",
       "      <td>1194</td>\n",
       "      <td>1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06001400200</td>\n",
       "      <td>46</td>\n",
       "      <td>110</td>\n",
       "      <td>2</td>\n",
       "      <td>538</td>\n",
       "      <td>650</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06001400300</td>\n",
       "      <td>67</td>\n",
       "      <td>416</td>\n",
       "      <td>7</td>\n",
       "      <td>1139</td>\n",
       "      <td>1562</td>\n",
       "      <td>1629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06001400400</td>\n",
       "      <td>57</td>\n",
       "      <td>391</td>\n",
       "      <td>3</td>\n",
       "      <td>777</td>\n",
       "      <td>1171</td>\n",
       "      <td>1228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06001400500</td>\n",
       "      <td>56</td>\n",
       "      <td>342</td>\n",
       "      <td>6</td>\n",
       "      <td>614</td>\n",
       "      <td>962</td>\n",
       "      <td>1018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9108</th>\n",
       "      <td>06115040902</td>\n",
       "      <td>759</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9109</th>\n",
       "      <td>06115041001</td>\n",
       "      <td>579</td>\n",
       "      <td>90</td>\n",
       "      <td>360</td>\n",
       "      <td>1807</td>\n",
       "      <td>2257</td>\n",
       "      <td>2836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9110</th>\n",
       "      <td>06115041002</td>\n",
       "      <td>270</td>\n",
       "      <td>155</td>\n",
       "      <td>628</td>\n",
       "      <td>1771</td>\n",
       "      <td>2554</td>\n",
       "      <td>2824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9111</th>\n",
       "      <td>06115041101</td>\n",
       "      <td>373</td>\n",
       "      <td>86</td>\n",
       "      <td>596</td>\n",
       "      <td>903</td>\n",
       "      <td>1585</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9112</th>\n",
       "      <td>06115041102</td>\n",
       "      <td>189</td>\n",
       "      <td>67</td>\n",
       "      <td>354</td>\n",
       "      <td>474</td>\n",
       "      <td>895</td>\n",
       "      <td>1084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9113 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CENSUSCODE  OTHER_OTHER  RESIDENTIAL_MULTI FAMILY  RESIDENTIAL_OTHER  \\\n",
       "0     06001400100          104                        22                 10   \n",
       "1     06001400200           46                       110                  2   \n",
       "2     06001400300           67                       416                  7   \n",
       "3     06001400400           57                       391                  3   \n",
       "4     06001400500           56                       342                  6   \n",
       "...           ...          ...                       ...                ...   \n",
       "9108  06115040902          759                         0                  0   \n",
       "9109  06115041001          579                        90                360   \n",
       "9110  06115041002          270                       155                628   \n",
       "9111  06115041101          373                        86                596   \n",
       "9112  06115041102          189                        67                354   \n",
       "\n",
       "      RESIDENTIAL_SINGLE FAMILY  TOTAL_RESIDENTIAL  TOTAL_BUILDING  \n",
       "0                          1162               1194            1298  \n",
       "1                           538                650             696  \n",
       "2                          1139               1562            1629  \n",
       "3                           777               1171            1228  \n",
       "4                           614                962            1018  \n",
       "...                         ...                ...             ...  \n",
       "9108                          0                  0             759  \n",
       "9109                       1807               2257            2836  \n",
       "9110                       1771               2554            2824  \n",
       "9111                        903               1585            1958  \n",
       "9112                        474                895            1084  \n",
       "\n",
       "[9113 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
