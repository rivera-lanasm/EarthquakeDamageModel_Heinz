{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import fiona\n",
    "import pyogrio\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD BUILDING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to download building data, extract it, then aggregate the data to count the number of building for each census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the webpage to scrape\n",
    "url = \"https://disasters.geoplatform.gov/USA_Structures/\"\n",
    "\n",
    "def fetch_state_links():\n",
    "    \"\"\"Fetches state names and their corresponding links from the webpage.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        return {link.text.strip(): link[\"href\"] for link in links if \"Deliverable\" in link[\"href\"]}\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return {}\n",
    "\n",
    "def get_link_by_state(state_name, state_links):\n",
    "    \"\"\"Returns the link for a given state name.\"\"\"\n",
    "    return state_links.get(state_name, \"State not found\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(state_name, state_links):\n",
    "    \"\"\"Downloads and extracts a ZIP file from the given URL.\"\"\"\n",
    "    url = get_link_by_state(state_name, state_links)\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    output_dir = os.path.join(parent_dir, 'Data', 'building_data_gdb')\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        zip_path = os.path.join(output_dir, f\"{state_name}_Structures.zip\")\n",
    "        \n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(f\"Downloaded, extracted, and deleted ZIP file for {state_name} to {output_dir}\")\n",
    "    else:\n",
    "        print(\"Failed to download the ZIP file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alabama': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Alabama/Deliverable202496AL.zip',\n",
       " 'Alaska': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Alaska/Deliverable20230728AK.zip',\n",
       " 'American Samoa': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/American+Samoa/Deliverable20230831AS.zip',\n",
       " 'Arizona': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Arizona/Deliverable20230502AZ.zip',\n",
       " 'Arkansas': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Arkansas/Deliverable20230630AR.zip',\n",
       " 'California': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/California/Deliverable20230728CA.zip',\n",
       " 'Colorado': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Colorado/Deliverable20230630CO.zip',\n",
       " 'Connecticut': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Connecticut/Deliverable20230502CT.zip',\n",
       " 'Delaware': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Delaware/Deliverable20230630DE.zip',\n",
       " 'D.C.': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/District+of+Columbia/Deliverable20230502DC.zip',\n",
       " 'Guam': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Guam/Deliverable20230831GU.zip',\n",
       " 'Florida': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Florida/Deliverable202496FL.zip',\n",
       " 'Georgia': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Georgia/Deliverable202496GA.zip',\n",
       " 'Hawaii': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Hawaii/Deliverable20230526HI.zip',\n",
       " 'Idaho': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Idaho/Deliverable20230526ID.zip',\n",
       " 'Illinois': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Illinois/Deliverable20230831IL.zip',\n",
       " 'Indiana': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Indiana/Deliverable20230502IN.zip',\n",
       " 'Iowa': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Iowa/Deliverable20230502IA.zip',\n",
       " 'Kansas': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Kansas/Deliverable20230630KS.zip',\n",
       " 'Kentucky': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Kentucky/Deliverable20230728KY.zip',\n",
       " 'Louisiana': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Louisiana/Deliverable202496LA.zip',\n",
       " 'Maine': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Maine/Deliverable20230502ME.zip',\n",
       " 'Maryland': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Maryland/Deliverable20230728MD.zip',\n",
       " 'Massachusetts': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Massachusetts/Deliverable20230502MA.zip',\n",
       " 'Michigan': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Michigan/Deliverable20230526MI.zip',\n",
       " 'Minnesota': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Minnesota/Deliverable20230728MN.zip',\n",
       " 'Missouri': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Missouri/Deliverable20230728MO.zip',\n",
       " 'Mississippi': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Mississippi/Deliverable202496MS.zip',\n",
       " 'Montana': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Montana/Deliverable20230502MT.zip',\n",
       " 'Nebraska': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Nebraska/Deliverable20230831NE.zip',\n",
       " 'Nevada': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Nevada/Deliverable20230526NV.zip',\n",
       " 'New Hampshire': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+Hampshire/Deliverable20230526NH.zip',\n",
       " 'New Jersey': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+Jersey/Deliverable20230502NJ.zip',\n",
       " 'New Mexico': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+Mexico/Deliverable20230502NM.zip',\n",
       " 'New York': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/New+York/Deliverable20230502NY.zip',\n",
       " 'North Carolina': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/North+Carolina/Deliverable202496NC.zip',\n",
       " 'North Dakota': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/North+Dakota/Deliverable20230728ND.zip',\n",
       " 'Northern Mariana Islands': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Northern+Mariana+Islands/Deliverable20230831MP.zip',\n",
       " 'Ohio': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Ohio/Deliverable20230502OH.zip',\n",
       " 'Oklahoma': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Oklahoma/Deliverable20231003OK.zip',\n",
       " 'Oregon': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Oregon/Deliverable20230526OR.zip',\n",
       " 'Pennsylvania': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Pennsylvania/Deliverable20230831PA.zip',\n",
       " 'Puerto Rico': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Puerto+Rico/Deliverable20230630PR.zip',\n",
       " 'Rhode Island': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Rhode+Island/Deliverable20230502RI.zip',\n",
       " 'South Carolina': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/South+Carolina/Deliverable202496SC.zip',\n",
       " 'South Dakota': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/South+Dakota/Deliverable20230728SD.zip',\n",
       " 'Tennessee': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Tennessee/Deliverable20230502TN.zip',\n",
       " 'Texas': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Texas/Deliverable202496TX.zip',\n",
       " 'Utah': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Utah/Deliverable20230502UT.zip',\n",
       " 'Vermont': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Vermont/Deliverable20230502VT.zip',\n",
       " 'Virgin Islands': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Virgin+Islands/Deliverable20230630VI.zip',\n",
       " 'Virginia': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Virginia/Deliverable202496VA.zip',\n",
       " 'Washington': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Washington/Deliverable20230831WA.zip',\n",
       " 'West Virginia': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/West+Virginia/Deliverable20230831WV.zip',\n",
       " 'Wisconsin': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Wisconsin/Deliverable20230728WI.zip',\n",
       " 'Wyoming': 'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/USA_Structures/Wyoming/Deliverable20230728WY.zip'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_links = fetch_state_links()\n",
    "state_links\n",
    "# download_and_extract_zip(\"Alaska\", state_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded, extracted, and deleted ZIP file for Alabama to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Alaska to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for American Samoa to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Arizona to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Arkansas to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for California to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Colorado to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Connecticut to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Delaware to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for D.C. to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Guam to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Florida to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Georgia to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Hawaii to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Idaho to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Illinois to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Indiana to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Iowa to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Kansas to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Kentucky to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Louisiana to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Maine to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Maryland to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Massachusetts to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Michigan to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Minnesota to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Missouri to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Mississippi to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Montana to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Nebraska to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Nevada to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New Hampshire to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New Jersey to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New Mexico to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for New York to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for North Carolina to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for North Dakota to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Northern Mariana Islands to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Ohio to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Oklahoma to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Oregon to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Pennsylvania to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Puerto Rico to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Rhode Island to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for South Carolina to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for South Dakota to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Tennessee to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Texas to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Utah to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Vermont to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Virgin Islands to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Virginia to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Washington to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for West Virginia to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Wisconsin to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Wyoming to c:\\Users\\ysurya\\Documents\\EarthquakeDamageModel_Heinz\\Data\\building_data_gdb\n"
     ]
    }
   ],
   "source": [
    "for state in state_links:\n",
    "    download_and_extract_zip(state, state_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building_data_directory(stateid):\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # get parent directory\n",
    "    parent_dir = os.path.dirname(cwd)\n",
    "    # get the building data directory\n",
    "    building_data_directory = os.path.join(parent_dir, 'Data', 'building_data_gdb')\n",
    "    # find all folder in the building data directory\n",
    "    folders = [f for f in os.listdir(building_data_directory) if os.path.isdir(os.path.join(building_data_directory, f))]\n",
    "    # get the folder that ends with stateid\n",
    "    stateid_dir= [f for f in folders if f.endswith(f'{stateid}')][0]\n",
    "\n",
    "    return os.path.join(building_data_directory, stateid_dir, f'{stateid}_Structures.gdb')\n",
    "\n",
    "def get_building_data_csv(stateid):\n",
    "    building_data_directory = get_building_data_directory(stateid)\n",
    "\n",
    "    # get the csv file\n",
    "    return os.path.join(building_data_directory, f'{stateid}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if false makedir\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    else:\n",
    "        print(f\"Directory {directory} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read only the specified columns\n",
    "def read_cols(path):\n",
    "    cols = ['BUILD_ID', 'OCC_CLS', 'PRIM_OCC', 'CENSUSCODE', 'LONGITUDE', 'LATITUDE']\n",
    "    return gpd.read_file(path, columns=cols)\n",
    "    \n",
    "# only read specific columns, it can reduce the memory usage and time for each state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a csv file for a state is exists\n",
    "    # if exists, read it\n",
    "    # if not, check if the gdb file exists\n",
    "    # if exists, read it\n",
    "def read_building_data(stateid):\n",
    "    building_data_directory = get_building_data_directory(stateid)\n",
    "\n",
    "    # get the csv file\n",
    "    csv_path = get_building_data_csv(stateid)\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Reading {csv_path}\")\n",
    "        return gpd.read_file(csv_path)\n",
    "    else:\n",
    "        print(f\"{csv_path} does not exist.\")\n",
    "        gdb_path = os.path.join(building_data_directory)\n",
    "        if os.path.exists(gdb_path):\n",
    "            print(f\"Reading {gdb_path}\")\n",
    "            return read_cols(gdb_path)\n",
    "        else:\n",
    "            print(f\"{gdb_path} does not exist.\")\n",
    "            print(\"Please download the gdb file from the USGS website.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_data = [\n",
    "    #(\"Alabama\", \"AL\"), (\"Alaska\", \"AK\"), (\"Arizona\", \"AZ\"), (\"Arkansas\", \"AR\"),\n",
    "    (\"California\", \"CA\"), (\"Colorado\", \"CO\"), (\"Connecticut\", \"CT\"), (\"Delaware\", \"DE\"),\n",
    "    (\"Florida\", \"FL\"), (\"Georgia\", \"GA\"), (\"Hawaii\", \"HI\"), (\"Idaho\", \"ID\"),\n",
    "    (\"Illinois\", \"IL\"), (\"Indiana\", \"IN\"), (\"Iowa\", \"IA\"), (\"Kansas\", \"KS\"),\n",
    "    (\"Kentucky\", \"KY\"), (\"Louisiana\", \"LA\"), (\"Maine\", \"ME\"), (\"Maryland\", \"MD\"),\n",
    "    (\"Massachusetts\", \"MA\"), (\"Michigan\", \"MI\"), (\"Minnesota\", \"MN\"), (\"Mississippi\", \"MS\"),\n",
    "    (\"Missouri\", \"MO\"), (\"Montana\", \"MT\"), (\"Nebraska\", \"NE\"), (\"Nevada\", \"NV\"),\n",
    "    (\"New Hampshire\", \"NH\"), (\"New Jersey\", \"NJ\"), (\"New Mexico\", \"NM\"), (\"New York\", \"NY\"),\n",
    "    (\"North Carolina\", \"NC\"), (\"North Dakota\", \"ND\"), (\"Ohio\", \"OH\"), (\"Oklahoma\", \"OK\"),\n",
    "    (\"Oregon\", \"OR\"), (\"Pennsylvania\", \"PA\"), (\"Rhode Island\", \"RI\"), (\"South Carolina\", \"SC\"),\n",
    "    (\"South Dakota\", \"SD\"), (\"Tennessee\", \"TN\"), (\"Texas\", \"TX\"), (\"Utah\", \"UT\"),\n",
    "    (\"Vermont\", \"VT\"), (\"Virginia\", \"VA\"), (\"Washington\", \"WA\"), (\"West Virginia\", \"WV\"),\n",
    "    (\"Wisconsin\", \"WI\"), (\"Wyoming\", \"WY\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATE BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remap OCC_CLS and PRIM_OCC\n",
    "def remap_occupancy_classes(gdf):\n",
    "    # Define the mapping dictionaries\n",
    "    building_data = gdf[['BUILD_ID', 'OCC_CLS', 'PRIM_OCC', 'CENSUSCODE', 'LONGITUDE', 'LATITUDE']]\n",
    "    # mapping the occupancy class\n",
    "    mapping = {\n",
    "        'Agriculture':'OTHER', 'Education':'OTHER', 'Residential':'RESIDENTIAL', 'Unclassified':'OTHER',\n",
    "        'Commercial':'OTHER', 'Government':'OTHER', 'Industrial':'OTHER', 'Utility and Misc':'OTHER',\n",
    "        'Assembly':'OTHER'\n",
    "    }\n",
    "    building_data['OCC_CLS'] = building_data['OCC_CLS'].map(mapping)\n",
    "\n",
    "    # mapping the primary occupancy\n",
    "    mapping = {i:'OTHER' for i in building_data['PRIM_OCC'].unique() if i not in ['Single Family Dwelling', 'Multi - Family Dwelling']}\n",
    "    residential = {'Single Family Dwelling':'SINGLE FAMILY', 'Multi - Family Dwelling':'MULTI FAMILY'}\n",
    "    mapping.update(residential)\n",
    "    building_data['PRIM_OCC'] = building_data['PRIM_OCC'].map(mapping)\n",
    "    return building_data\n",
    "\n",
    "\n",
    "# function to aggregate the building counts by GEODI, OCC_CLS, PRIM_OCC\n",
    "def aggregate_building_counts(gdf):\n",
    "    building_data = remap_occupancy_classes(gdf)\n",
    "    # group by GEODI, OCC_CLS, PRIM_OCC and sum the counts\n",
    "    count_building_data = building_data.groupby(['CENSUSCODE', 'OCC_CLS', 'PRIM_OCC']).agg({'BUILD_ID':'count'}).reset_index()\n",
    "    # rename the columns\n",
    "    count_building_data = count_building_data.rename(columns={'BUILD_ID':'COUNT'})\n",
    "    return count_building_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_building_data(count_building_data):\n",
    "    df = count_building_data.copy()\n",
    "\n",
    "    # Create a pivot table\n",
    "    df_pivot = df.pivot_table(index=\"CENSUSCODE\", columns=[\"OCC_CLS\", \"PRIM_OCC\"], values=\"COUNT\", aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "    # Flatten MultiIndex columns\n",
    "    df_pivot.columns = [f\"{col[0]}_{col[1]}\" for col in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states_data:\n",
    "    state_name, stateid = state\n",
    "    print(f\"Reading building data for {state_name}\")\n",
    "    gdf = read_building_data(stateid)\n",
    "    if gdf is not None:\n",
    "        count_building_data = aggregate_building_counts(gdf)\n",
    "        df_pivot = pivot_building_data(count_building_data)\n",
    "        output_dir = os.path.join(os.path.dirname(os.getcwd()), 'Data', 'building_data_csv')\n",
    "        create_directory_if_not_exists(output_dir)\n",
    "        output_path = os.path.join(output_dir, f\"{stateid}_building_data.csv\")\n",
    "        df_pivot.to_csv(output_path, index=False)\n",
    "        print(f\"Saved building data for {state_name} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading building data for California\n",
      "/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_gdb/Deliverable20230728CA/CA_Structures.gdb/CA.csv does not exist.\n",
      "Reading /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_gdb/Deliverable20230728CA/CA_Structures.gdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_65276/4015162652.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['OCC_CLS'] = building_data['OCC_CLS'].map(mapping)\n",
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_65276/4015162652.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['PRIM_OCC'] = building_data['PRIM_OCC'].map(mapping)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['SQFEET'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m gdf \u001b[38;5;241m=\u001b[39m read_building_data(stateid)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     count_building_data \u001b[38;5;241m=\u001b[39m \u001b[43maggregate_building_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     df_pivot \u001b[38;5;241m=\u001b[39m pivot_building_data(count_building_data)\n\u001b[1;32m      7\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mgetcwd()), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuilding_data_csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36maggregate_building_counts\u001b[0;34m(gdf)\u001b[0m\n\u001b[1;32m     23\u001b[0m building_data \u001b[38;5;241m=\u001b[39m remap_occupancy_classes(gdf)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# group by GEODI, OCC_CLS, PRIM_OCC and sum the counts\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m count_building_data \u001b[38;5;241m=\u001b[39m \u001b[43mbuilding_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCENSUSCODE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOCC_CLS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPRIM_OCC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBUILD_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSQFEET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# rename the columns\u001b[39;00m\n\u001b[1;32m     27\u001b[0m count_building_data \u001b[38;5;241m=\u001b[39m count_building_data\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBUILD_ID\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOUNT\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.12/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.12/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.12/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.12/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.12/site-packages/pandas/core/apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/capstone/lib/python3.12/site-packages/pandas/core/apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['SQFEET'] do not exist\""
     ]
    }
   ],
   "source": [
    "state_name, stateid = (\"California\", \"CA\")\n",
    "print(f\"Reading building data for {state_name}\")\n",
    "gdf = read_building_data(stateid)\n",
    "if gdf is not None:\n",
    "    count_building_data = aggregate_building_counts(gdf)\n",
    "    df_pivot = pivot_building_data(count_building_data)\n",
    "    output_dir = os.path.join(os.path.dirname(os.getcwd()), 'Data', 'building_data_csv')\n",
    "    create_directory_if_not_exists(output_dir)\n",
    "    output_path = os.path.join(output_dir, f\"{stateid}_building_data.csv\")\n",
    "    df_pivot.to_csv(output_path, index=False)\n",
    "    print(f\"Saved building data for {state_name} to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
