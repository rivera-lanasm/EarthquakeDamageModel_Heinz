{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fiona\n",
    "import pyogrio\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ EVENT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_event_data(eventid = 'nc72282711'):\n",
    "    \"\"\"\n",
    "    Read event data from a GPKG file.\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    event_dir = os.path.join(parent_dir, 'ShakeMaps', eventid)\n",
    "\n",
    "    # Update with the actual path\n",
    "    GPKG_PATH = os.path.join(event_dir, \"eqmodel_outputs.gpkg\")\n",
    "\n",
    "    # Read the layer you want to inspect\n",
    "    # tract_shakemap_mmi, tract_shakemap_pga, tract_shakemap_pgv --> same idea\n",
    "    gdf = gpd.read_file(GPKG_PATH, layer=\"tract_shakemap_pga\")\n",
    "    # make sure that only row that is not nan is the one we want\n",
    "    columns = gdf.columns\n",
    "    gdf = gdf[[columns[0], columns[1], columns[2], columns[3], columns[-1]]]\n",
    "    \n",
    "    return gdf.loc[gdf[columns[1]].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a csv file for a state is exists\n",
    "    # if exists, read it\n",
    "    # if not, check if the gdb file exists\n",
    "    # if exists, read it\n",
    "def read_building_count_by_tract():\n",
    "    \"\"\"\n",
    "    Read building count data from a CSV file.\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    # Update with the actual path\n",
    "    CSV_PATH = os.path.join(parent_dir, 'Data', 'building_data_csv', \"aggregated_building_data.csv\")\n",
    "    # check if the file exists\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        print(f\"CSV file for Building count data is not available.\")\n",
    "        return None\n",
    "    else:\n",
    "        gdf = pd.read_csv(CSV_PATH, dtype={'CENSUSCODE': str})\n",
    "        gdf['CENSUSCODE'] = np.where(gdf['CENSUSCODE'].str.len() == 11, gdf['CENSUSCODE'], \"0\"+gdf['CENSUSCODE'])\n",
    "        return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERSECT WITH BUILDING STOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building_stock_data():\n",
    "    \"\"\"\n",
    "    2. Check if the csv file exists\n",
    "    3. If not, create the folder aand copy the csv file\n",
    "    4. If exists, read the csv file\n",
    "    \"\"\"\n",
    "\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    # check if the folder exists\n",
    "    CSV_PATH = os.path.join(parent_dir, 'Data', 'building_stock_data', 'Building_Percentages_Per_Tract_ALLSTATES.csv')\n",
    "    \n",
    "\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        print(f\"Building stock data exists at {CSV_PATH}\")\n",
    "        gdf = gpd.read_file(CSV_PATH)\n",
    "        gdf['CENSUSCODE'] = np.where(gdf['Tract'].str.len() == 11, gdf['Tract'], \"0\"+gdf['Tract'])\n",
    "\n",
    "    else:\n",
    "        print(f\"Building stock data does not exist at {CSV_PATH}\")\n",
    "        # create or download the files\n",
    "        pass\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN COUNT BUILDING DATA AND BUILDING STOCK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take df_pivot and building_stock and merge them\n",
    "def count_building_proportion(building_count, building_stock):\n",
    "    # merge the dataframes\n",
    "    merged_df = pd.merge(building_count, building_stock, on='CENSUSCODE', how='left')\n",
    "    merged_df.drop(columns=['Tract'], axis=1, inplace=True)\n",
    "    merged_df.drop(columns=['field_1'], axis=1, inplace=True)\n",
    "    merged_df.bfill(inplace=True)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE OUTPUT TO EVENT DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save GeoDataFrame to GeoPackage (Overwriting mode)\n",
    "def save_to_geopackage(gdf, layer_name=\"tract_shakemap_pga\", eventid = 'nc72282711'):\n",
    "    \"\"\"\n",
    "    Saves a GeoDataFrame to the GeoPackage, overwriting the existing layer.\n",
    "\n",
    "    Args:\n",
    "        gdf (GeoDataFrame): The GeoDataFrame to save.\n",
    "        layer_name (str): The name of the layer in the GeoPackage.\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    event_dir = os.path.join(parent_dir, 'ShakeMaps', eventid)\n",
    "\n",
    "    # Update with the actual path\n",
    "    GPKG_PATH = os.path.join(event_dir, \"eqmodel_outputs.gpkg\")\n",
    "\n",
    "\n",
    "    gdf.to_file(GPKG_PATH, layer=layer_name, driver=\"GPKG\", mode=\"w\")\n",
    "    print(f\"Saved {layer_name} to {GPKG_PATH} (overwritten).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_clip_analysis(eventid):\n",
    "    # overall work flow\n",
    "    # 1. Read the event data\n",
    "    print(f\"1. Reading event data for event ID: {eventid}\")\n",
    "    eventdata = read_event_data(eventid)\n",
    "    # 2. Read the building count data\n",
    "    print(\"2. Reading building count data...\")\n",
    "    building_count = read_building_count_by_tract()\n",
    "    # 3. Read the building stock data\n",
    "    print(\"3. Reading building stock data...\")\n",
    "    building_stock = get_building_stock_data()\n",
    "    # 4. Merge the building count and building stock data\n",
    "    print(\"4. Merging building count and building stock data...\")\n",
    "    df_output = count_building_proportion(building_count, building_stock)\n",
    "    # 5. Merge the event data and the merged building count and building stock data\n",
    "    print(\"5. Merging event data with building data...\")\n",
    "    final_output = pd.merge(eventdata, df_output, left_on='GEOID', right_on='CENSUSCODE', how='left')\n",
    "    final_output.ffill(inplace=True)\n",
    "    final_output.drop(columns=['CENSUSCODE'], axis=1, inplace=True)\n",
    "    final_output.head()\n",
    "    # 6. Save the final output to the GeoPackage\n",
    "    print(\"6. Saving final output to GeoPackage...\")\n",
    "    layer_name = \"tract_shakemap_pga\"\n",
    "    save_to_geopackage(final_output, layer_name, eventid)\n",
    "    print(f\"Building clip analysis completed for event ID: {eventid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reading event data for event ID: nc72282711\n",
      "2. Reading building count data...\n",
      "3. Reading building stock data...\n",
      "Building stock data exists at /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_stock_data/Building_Percentages_Per_Tract_ALLSTATES.csv\n",
      "4. Merging building count and building stock data...\n",
      "5. Merging event data with building data...\n",
      "6. Saving final output to GeoPackage...\n",
      "Saved tract_shakemap_pga to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/ShakeMaps/nc72282711/eqmodel_outputs.gpkg (overwritten).\n",
      "Building clip analysis completed for event ID: nc72282711\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    # Read building data and save to GeoPackage\n",
    "    # building_count = read_building_count_by_tract()\n",
    "    # building_stock = get_building_stock_data()\n",
    "    # df_output = count_building_proportion(building_count, building_stock)\n",
    "    # save_to_geopackage(df_output, layer_name=\"building_data\")\n",
    "    \n",
    "    # Perform building clip analysis for a specific event ID\n",
    "    eventid = 'nc72282711'\n",
    "    building_clip_analysis(eventid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
