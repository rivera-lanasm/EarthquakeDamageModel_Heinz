{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ EVENT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_event_data(parent_dir, eventid = 'nc72282711'):\n",
    "    \"\"\"\n",
    "    Read event data from a GPKG file.\n",
    "    \"\"\"\n",
    "    # parent_dir = os.path.dirname(os.getcwd())\n",
    "    event_dir = os.path.join(parent_dir, 'ShakeMaps', eventid)\n",
    "\n",
    "    # Update with the actual path\n",
    "    GPKG_PATH = os.path.join(event_dir, \"eqmodel_outputs.gpkg\")\n",
    "\n",
    "    # Read the layer you want to inspect\n",
    "    # tract_shakemap_mmi, tract_shakemap_pga, tract_shakemap_pgv --> same idea\n",
    "    gdf = gpd.read_file(GPKG_PATH, layer=\"tract_shakemap_pga\")\n",
    "    # make sure that only row that is not nan is the one we want\n",
    "    columns = gdf.columns\n",
    "    gdf = gdf[[columns[0], columns[1], columns[2], columns[3], columns[-1]]]\n",
    "    \n",
    "    # change to panda dataframe\n",
    "    pd_gdf = pd.DataFrame(gdf)\n",
    "    return pd_gdf.loc[pd_gdf[columns[1]].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a csv file for a state is exists\n",
    "    # if exists, read it\n",
    "    # if not, check if the gdb file exists\n",
    "    # if exists, read it\n",
    "def read_building_count_by_tract():\n",
    "    \"\"\"\n",
    "    Read building count data from a CSV file.\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    # Update with the actual path\n",
    "    CSV_PATH = os.path.join(parent_dir, 'Data', 'building_data_csv', \"aggregated_building_data.csv\")\n",
    "    # check if the file exists\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        print(f\"CSV file for Building count data is not available.\")\n",
    "        return None\n",
    "    else:\n",
    "        gdf = pd.read_csv(CSV_PATH, dtype={'CENSUSCODE': str})\n",
    "        gdf['CENSUSCODE'] = np.where(gdf['CENSUSCODE'].str.len() == 11, gdf['CENSUSCODE'], \"0\"+gdf['CENSUSCODE'])\n",
    "        return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERSECT WITH BUILDING STOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building_stock_data():\n",
    "    \"\"\"\n",
    "    2. Check if the csv file exists\n",
    "    3. If not, create the folder aand copy the csv file\n",
    "    4. If exists, read the csv file\n",
    "    \"\"\"\n",
    "\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    # check if the folder exists\n",
    "    CSV_PATH = os.path.join(parent_dir, 'Data', 'building_stock_data', 'Building_Percentages_Per_Tract_ALLSTATES.csv')\n",
    "    \n",
    "    # Change data types\n",
    "    cols = ['W1', 'W2', 'S1L', 'S1M', 'S1H', 'S2L', 'S2M',\n",
    "       'S2H', 'S3', 'S4L', 'S4M', 'S4H', 'S5L', 'S5M', 'S5H', 'C1L', 'C1M',\n",
    "       'C1H', 'C2L', 'C2M', 'C2H', 'C3L', 'C3M', 'C3H', 'PC1', 'PC2L', 'PC2M',\n",
    "       'PC2H', 'RM1L', 'RM1M', 'RM2L', 'RM2M', 'RM2H', 'URML', 'URMM', 'MH',\n",
    "       'Total']\n",
    "    # create a library for data type change\n",
    "    dtypes = {}\n",
    "    for col in cols:\n",
    "        dtypes[col] = 'float64'\n",
    "    dtypes['Tract'] = 'str'\n",
    "    \n",
    "    if os.path.exists(CSV_PATH):\n",
    "        print(f\"Building stock data exists at {CSV_PATH}\")\n",
    "        gdf = pd.read_csv(CSV_PATH, dtype=dtypes)\n",
    "        gdf = gdf.drop(columns=['Unnamed: 0'])\n",
    "        gdf['CENSUSCODE'] = np.where(gdf['Tract'].str.len() == 11, gdf['Tract'], \"0\"+gdf['Tract'])\n",
    "\n",
    "    else:\n",
    "        print(f\"Building stock data does not exist at {CSV_PATH}\")\n",
    "        # create or download the files\n",
    "        pass\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN COUNT BUILDING DATA AND BUILDING STOCK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take df_pivot and building_stock and merge them\n",
    "def count_building_proportion(building_count, building_stock):\n",
    "    # merge the dataframes\n",
    "    merged_df = pd.merge(building_count, building_stock, on='CENSUSCODE', how='left')\n",
    "    merged_df.drop(columns=['Tract'], axis=1, inplace=True)\n",
    "    merged_df.drop(columns=['STATE_ID'], axis=1, inplace=True)\n",
    "    #merged_df.drop(columns=['field_1'], axis=1, inplace=True)\n",
    "    merged_df.bfill(inplace=True)\n",
    "\n",
    "    # calculate the number of each building type\n",
    "    cols = ['W1', 'W2', 'S1L', 'S1M', 'S1H', 'S2L', 'S2M',\n",
    "       'S2H', 'S3', 'S4L', 'S4M', 'S4H', 'S5L', 'S5M', 'S5H', 'C1L', 'C1M',\n",
    "       'C1H', 'C2L', 'C2M', 'C2H', 'C3L', 'C3M', 'C3H', 'PC1', 'PC2L', 'PC2M',\n",
    "       'PC2H', 'RM1L', 'RM1M', 'RM2L', 'RM2M', 'RM2H', 'URML', 'URMM', 'MH']\n",
    "    for col in cols:\n",
    "        merged_df[f\"{col}\"] = round(merged_df[col]/merged_df['Total'] * merged_df['TOTAL_BUILDING_COUNT'])\n",
    "    \n",
    "    # drop the proportion columns\n",
    "    #merged_df.drop(columns=cols, axis=1, inplace=True)\n",
    "    merged_df.drop(columns=['Total'], axis=1, inplace=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE OUTPUT TO EVENT DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save GeoDataFrame to GeoPackage (Overwriting mode)\n",
    "def save_to_geopackage(gdf, layer_name=\"tract_shakemap_pga\", eventid = 'nc72282711'):\n",
    "    \"\"\"\n",
    "    Saves a GeoDataFrame to the GeoPackage, overwriting the existing layer.\n",
    "\n",
    "    Args:\n",
    "        gdf (GeoDataFrame): The GeoDataFrame to save.\n",
    "        layer_name (str): The name of the layer in the GeoPackage.\n",
    "    \"\"\"\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    event_dir = os.path.join(parent_dir, 'ShakeMaps', eventid)\n",
    "\n",
    "    # Update with the actual path\n",
    "    GPKG_PATH = os.path.join(event_dir, \"eqmodel_outputs.gpkg\")\n",
    "\n",
    "    # change gdf to geodataframe\n",
    "    gdf = gpd.GeoDataFrame(gdf, geometry=gdf['geometry'])\n",
    "\n",
    "    gdf.to_file(GPKG_PATH, layer=layer_name, driver=\"GPKG\", mode=\"w\")\n",
    "    print(f\"Saved {layer_name} to {GPKG_PATH} (overwritten).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_final_df(eventdata, df_output):\n",
    "    \"\"\"\n",
    "    Merge the final dataframe with the event data.\"\"\"\n",
    "    final_output = pd.merge(eventdata, df_output, left_on='GEOID', right_on='CENSUSCODE', how='left')\n",
    "    final_output = final_output.ffill(axis=0)\n",
    "    final_output.drop(columns=['CENSUSCODE'], axis=1, inplace=True)\n",
    "    return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_clip_analysis(parent_dir, eventid):\n",
    "    # overall work flow\n",
    "    # 1. Read the event data\n",
    "    print(f\"1. Reading event data for event ID: {eventid}\")\n",
    "    eventdata = read_event_data(parent_dir, eventid)\n",
    "\n",
    "    # 2. Read the building count data\n",
    "    print(\"2. Reading building count data...\")\n",
    "    building_count = read_building_count_by_tract()\n",
    "    # 3. Read the building stock data\n",
    "    print(\"3. Reading building stock data...\")\n",
    "    building_stock = get_building_stock_data()\n",
    "    # 4. Merge the building count and building stock data\n",
    "    print(\"4. Merging building count and building stock data...\")\n",
    "    df_output = count_building_proportion(building_count, building_stock)\n",
    "\n",
    "    # 5. Merge the event data and the merged building count and building stock data\n",
    "    print(\"5. Merging event data with building data...\")\n",
    "    final_output = merge_final_df(eventdata, df_output)\n",
    "  \n",
    "    # 6. Save the final output to the GeoPackage\n",
    "    print(\"6. Saving final output to GeoPackage...\")\n",
    "    layer_name = \"tract_shakemap_pga\"\n",
    "    save_to_geopackage(final_output, layer_name, eventid)\n",
    "    print(f\"Building clip analysis completed for event ID: {eventid}\")\n",
    "    # 7. Save the final output to a CSV file\n",
    "    print(\"Saving final output to CSV...\")\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    event_dir = os.path.join(parent_dir, 'ShakeMaps', eventid)\n",
    "    final_output_csv_path = os.path.join(event_dir, \"o3_building_clip_analysis.csv\")\n",
    "    final_output.to_csv(final_output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoPandas version: 1.0.1\n",
      "Pandas version: 2.2.2\n",
      "1. Reading event data for event ID: nc72282711\n",
      "2. Reading building count data...\n",
      "3. Reading building stock data...\n",
      "Building stock data exists at /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_stock_data/Building_Percentages_Per_Tract_ALLSTATES.csv\n",
      "4. Merging building count and building stock data...\n",
      "5. Merging event data with building data...\n",
      "6. Saving final output to GeoPackage...\n",
      "Saved tract_shakemap_pga to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/ShakeMaps/nc72282711/eqmodel_outputs.gpkg (overwritten).\n",
      "Building clip analysis completed for event ID: nc72282711\n",
      "Saving final output to CSV...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    # Read building data and save to GeoPackage\n",
    "    # building_count = read_building_count_by_tract()\n",
    "    # building_stock = get_building_stock_data()\n",
    "    # df_output = count_building_proportion(building_count, building_stock)\n",
    "    # save_to_geopackage(df_output, layer_name=\"building_data\")\n",
    "    print(\"GeoPandas version:\", gpd.__version__)\n",
    "    print(\"Pandas version:\", pd.__version__)\n",
    "\n",
    "    # Perform building clip analysis for a specific event ID\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    eventid = 'nc72282711'\n",
    "    building_clip_analysis(parent_dir, eventid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
