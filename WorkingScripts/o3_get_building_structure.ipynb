{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import fiona\n",
    "import pyogrio\n",
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if directory exist if not make it, return path\n",
    "def make_data_path():\n",
    "    \"\"\"Create directories for data storage if they do not exist.\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    parent = os.path.dirname(cwd)\n",
    "    data_path = os.path.join(parent, 'Data')\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    building_data_csv = os.path.join(data_path, 'building_data_csv')\n",
    "    building_data_gdb = os.path.join(data_path, 'building_data_gdb')\n",
    "    building_stock_data = os.path.join(data_path, 'building_stock_data')\n",
    "    if not os.path.exists(building_data_csv):\n",
    "        os.makedirs(building_data_csv)\n",
    "    if not os.path.exists(building_data_gdb):\n",
    "        os.makedirs(building_data_gdb)\n",
    "    if not os.path.exists(building_stock_data):\n",
    "        os.makedirs(building_stock_data)\n",
    "\n",
    "    return building_data_csv, building_data_gdb, building_stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD BUILDING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to download building data, extract it, then aggregate the data to count the number of building for each census tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_state_links():\n",
    "    \"\"\"Fetches state names and their corresponding links from the webpage.\"\"\"\n",
    "   \n",
    "    # URL of the webpage to scrape\n",
    "    url = \"https://disasters.geoplatform.gov/USA_Structures/\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        return {link.text.strip(): link[\"href\"] for link in links if \"Deliverable\" in link[\"href\"]}\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n",
    "        return {}\n",
    "\n",
    "def get_link_by_state(state_name, state_links):\n",
    "    \"\"\"Returns the link for a given state name.\"\"\"\n",
    "    return state_links.get(state_name, \"State not found\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(state_name, state_links):\n",
    "    \"\"\"Downloads and extracts a ZIP file from the given URL.\n",
    "    Keyword arguments:\n",
    "    state_name -- Name of the state\n",
    "    state_links -- Corresponding links for each state\n",
    "    \"\"\"\n",
    "    url = get_link_by_state(state_name, state_links)\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    output_dir = os.path.join(parent_dir, 'Data', 'building_data_gdb')\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        zip_path = os.path.join(output_dir, f\"{state_name}_Structures.zip\")\n",
    "        \n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_dir)\n",
    "        \n",
    "        os.remove(zip_path)\n",
    "        print(f\"Downloaded, extracted, and deleted ZIP file for {state_name} to {output_dir}\")\n",
    "    else:\n",
    "        print(\"Failed to download the ZIP file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdb_path_by_state(stateid):\n",
    "    \"\"\"Returns the path to the GDB file for a given state ID.\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    # get parent directory\n",
    "    parent_dir = os.path.dirname(cwd)\n",
    "    # get the building data directory\n",
    "    building_data_directory = os.path.join(parent_dir, 'Data', 'building_data_gdb')\n",
    "    # find all folder in the building data directory\n",
    "    folders = [f for f in os.listdir(building_data_directory) if os.path.isdir(os.path.join(building_data_directory, f))]\n",
    "    # get the folder that ends with stateid\n",
    "    stateid_dir= [f for f in folders if f.endswith(f'{stateid}')][0]\n",
    "\n",
    "    return os.path.join(building_data_directory, stateid_dir, f'{stateid}_Structures.gdb')\n",
    "\n",
    "def get_building_data_csv(stateid):\n",
    "    \"\"\"Returns the path to the CSV file for a given state ID.\"\"\"\n",
    "    building_data_directory = os.path.join(os.path.dirname(os.getcwd()), 'Data', 'building_data_csv')\n",
    "\n",
    "    # get the csv file\n",
    "    return os.path.join(building_data_directory, f'{stateid}_building_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read only the specified columns\n",
    "def read_cols(path):\n",
    "    \"\"\"Read the GDB file and return a GeoDataFrame with specified columns.\"\"\"\n",
    "    \n",
    "    cols = ['BUILD_ID', 'OCC_CLS', 'PRIM_OCC', 'CENSUSCODE', 'LONGITUDE', 'LATITUDE']\n",
    "    return gpd.read_file(path, columns=cols)\n",
    "    \n",
    "# only read specific columns, it can reduce the memory usage and time for each state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_building_data(stateid):\n",
    "    \"\"\"Check if the aggregated csv file exists for the given state ID.\n",
    "    If it does, return the file type and None. Do nothing.\n",
    "    If it does not, read the GDB file for the state and return the file type and the GeoDataFrame.\n",
    "    Kwargs:\n",
    "    stateid -- State ID\n",
    "    \"\"\"\n",
    "    \n",
    "    # gdb file path\n",
    "    building_data_directory = gdb_path_by_state(stateid)\n",
    "\n",
    "    # get the csv file\n",
    "    csv_path = get_building_data_csv(stateid)\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"Aggregated csv file found for {stateid}\")\n",
    "        return 'csv', None\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading {building_data_directory}\")\n",
    "        return 'gdb', read_cols(building_data_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATE BUILDING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remap OCC_CLS and PRIM_OCC\n",
    "def remap_occupancy_classes(gdf):\n",
    "    \"\"\"Remap the occupancy classes and primary occupancy from GDB files.\n",
    "    Kwargs:\n",
    "    gdf -- GeoDataFrame read from the GDB file\"\"\"\n",
    "\n",
    "    # Define the mapping dictionaries\n",
    "    building_data = gdf[['BUILD_ID', 'OCC_CLS', 'PRIM_OCC', 'CENSUSCODE', 'LONGITUDE', 'LATITUDE']]\n",
    "    # mapping the occupancy class\n",
    "    mapping = {\n",
    "        'Agriculture':'OTHER', 'Education':'OTHER', 'Residential':'RESIDENTIAL', 'Unclassified':'OTHER',\n",
    "        'Commercial':'OTHER', 'Government':'OTHER', 'Industrial':'OTHER', 'Utility and Misc':'OTHER',\n",
    "        'Assembly':'OTHER'\n",
    "    }\n",
    "    building_data['OCC_CLS'] = building_data['OCC_CLS'].map(mapping)\n",
    "\n",
    "    # mapping the primary occupancy\n",
    "    mapping = {i:'OTHER' for i in building_data['PRIM_OCC'].unique() if i not in ['Single Family Dwelling', 'Multi - Family Dwelling']}\n",
    "    residential = {'Single Family Dwelling':'SINGLE FAMILY', 'Multi - Family Dwelling':'MULTI FAMILY'}\n",
    "    mapping.update(residential)\n",
    "    building_data['PRIM_OCC'] = building_data['PRIM_OCC'].map(mapping)\n",
    "    return building_data\n",
    "\n",
    "\n",
    "# function to aggregate the building counts by GEODI, OCC_CLS, PRIM_OCC\n",
    "def aggregate_building_counts(gdf):\n",
    "    building_data = remap_occupancy_classes(gdf)\n",
    "    # group by GEODI, OCC_CLS, PRIM_OCC and sum the counts\n",
    "    count_building_data = building_data.groupby(['CENSUSCODE', 'OCC_CLS', 'PRIM_OCC']).agg({'BUILD_ID':'count'}).reset_index()\n",
    "    # rename the columns\n",
    "    count_building_data = count_building_data.rename(columns={'BUILD_ID':'COUNT'})\n",
    "    return count_building_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_building_data(count_building_data):\n",
    "    \"\"\"Pivot the building data to get the count of buildings by OCC_CLS and PRIM_OCC.\n",
    "    Kwargs:\n",
    "    count_building_data -- DataFrame with building counts aggregated by CENSUS CODE, OCC_CLS and PRIM_OCC\n",
    "    \"\"\"\n",
    "\n",
    "    df = count_building_data.copy()\n",
    "\n",
    "    # Create a pivot table\n",
    "    df_pivot = df.pivot_table(index=\"CENSUSCODE\", columns=[\"OCC_CLS\", \"PRIM_OCC\"], values=\"COUNT\", aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "    # Flatten MultiIndex columns\n",
    "    df_pivot.columns = [f\"{col[0]}_{col[1]}\" for col in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    df_pivot['TOTAL_RESIDENTIAL'] = df_pivot['RESIDENTIAL_MULTI FAMILY'] + df_pivot['RESIDENTIAL_SINGLE FAMILY'] + df_pivot['RESIDENTIAL_OTHER']\n",
    "    df_pivot['TOTAL_BUILDING'] = df_pivot['TOTAL_RESIDENTIAL'] + df_pivot['OTHER_OTHER']\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_building_data():\n",
    "    \"\"\"Aggregate the building data for all states and save to a csv file.\"\"\"\n",
    "    # list all files \n",
    "    path = os.path.join(os.path.dirname(os.getcwd()), 'Data', 'building_data_csv')\n",
    "    files = os.listdir(path)\n",
    "    # filter the files that ends with .csv\n",
    "    files = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "    # read all the csv files and concatenate them\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(path, file))\n",
    "        # get the state id from the file name\n",
    "        stateid = file.split('_')[0]\n",
    "        df['STATE_ID'] = stateid\n",
    "        dfs.append(df)\n",
    "    # concatenate the dataframes\n",
    "    building_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # drop OTHER_SINGLE FAMILY\n",
    "    building_data = building_data.drop(columns=['OTHER_SINGLE FAMILY'], errors='ignore')\n",
    "\n",
    "    # sum all building\n",
    "    building_data['TOTAL_BUILDING_COUNT'] = building_data['OTHER_OTHER']+building_data['RESIDENTIAL_MULTI FAMILY']+building_data['RESIDENTIAL_OTHER']+building_data['RESIDENTIAL_SINGLE FAMILY']\n",
    "    \n",
    "    # save the building data to a csv file\n",
    "    building_data.to_csv(os.path.join(path, 'aggregated_building_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def o3_get_building_structures():\n",
    "    \"\"\"Download and extract building data for all states from the given URL.\"\"\"\n",
    "\n",
    "    # get url from webpage\n",
    "    state_links = fetch_state_links()\n",
    "\n",
    "    # Iterate through the state names and download the corresponding ZIP files\n",
    "    i = 0\n",
    "    for state in state_links:\n",
    "        if i < len(state_links):\n",
    "            download_and_extract_zip(state, state_links)\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "\n",
    "    # states data\n",
    "    states_data = [\n",
    "        (\"Alabama\", \"AL\"), (\"Alaska\", \"AK\")\n",
    "        (\"Arizona\", \"AZ\"), (\"Arkansas\", \"AR\"), (\"California\", \"CA\"), (\"Colorado\", \"CO\"), (\"Connecticut\", \"CT\"), (\"Delaware\", \"DE\"),\n",
    "        (\"Florida\", \"FL\"), (\"Georgia\", \"GA\"), (\"Hawaii\", \"HI\"), (\"Idaho\", \"ID\"),\n",
    "        (\"Illinois\", \"IL\"), (\"Indiana\", \"IN\"), (\"Iowa\", \"IA\"), (\"Kansas\", \"KS\"),\n",
    "        (\"Kentucky\", \"KY\"), (\"Louisiana\", \"LA\"), (\"Maine\", \"ME\"), (\"Maryland\", \"MD\"),\n",
    "        (\"Massachusetts\", \"MA\"), (\"Michigan\", \"MI\"), (\"Minnesota\", \"MN\"), (\"Mississippi\", \"MS\"),\n",
    "        (\"Missouri\", \"MO\"), (\"Montana\", \"MT\"), (\"Nebraska\", \"NE\"), (\"Nevada\", \"NV\"),\n",
    "        (\"New Hampshire\", \"NH\"), (\"New Jersey\", \"NJ\"), (\"New Mexico\", \"NM\"), (\"New York\", \"NY\"),\n",
    "        (\"North Carolina\", \"NC\"), (\"North Dakota\", \"ND\"), (\"Ohio\", \"OH\"), (\"Oklahoma\", \"OK\"),\n",
    "        (\"Oregon\", \"OR\"), (\"Pennsylvania\", \"PA\"), (\"Rhode Island\", \"RI\"), (\"South Carolina\", \"SC\"),\n",
    "        (\"South Dakota\", \"SD\"), (\"Tennessee\", \"TN\"), (\"Texas\", \"TX\"), (\"Utah\", \"UT\"),\n",
    "        (\"Vermont\", \"VT\"), (\"Virginia\", \"VA\"), (\"Washington\", \"WA\"), (\"West Virginia\", \"WV\"),\n",
    "        (\"Wisconsin\", \"WI\"), (\"Wyoming\", \"WY\")]\n",
    "\n",
    "    # read the shapefiles for all states\n",
    "    for state_name, stateid in states_data:\n",
    "        print(f\"Reading building data for {state_name}\")\n",
    "        filetype, gdf = read_building_data(stateid)\n",
    "        if filetype == 'csv':\n",
    "            pass\n",
    "        elif filetype == 'gdb':\n",
    "            count_building_data = aggregate_building_counts(gdf)\n",
    "            df_pivot = pivot_building_data(count_building_data)\n",
    "            output_path = os.path.join(os.path.dirname(os.getcwd()), 'Data', 'building_data_csv', f\"{stateid}_building_data.csv\")\n",
    "            df_pivot.to_csv(output_path, index=False)\n",
    "            print(f\"Saved building data for {state_name} to {output_path}\")\n",
    "\n",
    "    # concatenate all the csv files\n",
    "    aggregate_building_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded, extracted, and deleted ZIP file for Alabama to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_gdb\n",
      "Downloaded, extracted, and deleted ZIP file for Alaska to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_gdb\n",
      "Reading building data for Alabama\n",
      "Reading /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_gdb/Deliverable202496AL/AL_Structures.gdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_43813/1288599890.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['OCC_CLS'] = building_data['OCC_CLS'].map(mapping)\n",
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_43813/1288599890.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['PRIM_OCC'] = building_data['PRIM_OCC'].map(mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved building data for Alabama to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_csv/AL_building_data.csv\n",
      "Reading building data for Alaska\n",
      "Reading /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_gdb/Deliverable20230728AK/AK_Structures.gdb\n",
      "Saved building data for Alaska to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/building_data_csv/AK_building_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_43813/1288599890.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['OCC_CLS'] = building_data['OCC_CLS'].map(mapping)\n",
      "/var/folders/26/f5bmf46n06g2nxwv03lg4yxm0000gn/T/ipykernel_43813/1288599890.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  building_data['PRIM_OCC'] = building_data['PRIM_OCC'].map(mapping)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # make the data path\n",
    "    building_data_csv, building_data_gdb, building_stock_data = make_data_path()\n",
    "    # download and extract the building data\n",
    "    o3_get_building_structures()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
