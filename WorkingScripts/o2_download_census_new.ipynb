{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# URL to the Census TIGER/TRACT 2019 directory\n",
    "\n",
    "BASE_URL = \"https://www2.census.gov/geo/tiger/TIGER2024/TRACT/\"\n",
    "\n",
    "def download_census(state_list, parent_dir):\n",
    "\n",
    "    # Local directories\n",
    "    download_folder = os.path.join(parent_dir, \"Data\", \"census_shp\")\n",
    "    extracted_folder = os.path.join(parent_dir, \"Data\", \"extracted_census_shp\")\n",
    "    merged_shapefile_folder = os.path.join(parent_dir, \"Data\", \"merged_shapefile\")\n",
    "    output_gpkg = os.path.join(merged_shapefile_folder, \"Nationwide_Tracts.gpkg\")\n",
    "\n",
    "    # Ensure directories exist\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "    os.makedirs(extracted_folder, exist_ok=True)\n",
    "    os.makedirs(merged_shapefile_folder, exist_ok=True)\n",
    "\n",
    "    # Fetch the webpage content\n",
    "    print(\"attempting to link to {}\".format(BASE_URL))\n",
    "    response = requests.get(BASE_URL)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to access {} --> response code {}\".format(BASE_URL, response.status_code))\n",
    "        raise ValueError\n",
    "    else:\n",
    "        print(\"parsing...\")\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Find all links ending with .zip\n",
    "    zip_links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.zip')]\n",
    "    zip_links = [link for link in zip_links if any(state in link for state in state_list)]\n",
    "    print(f\"Found {len(zip_links)} ZIP files. Starting download...\")\n",
    "\n",
    "    # 1) Download each ZIP file\n",
    "    print(\"Download Census Data to {}\".format(download_folder))\n",
    "    for i, zip_file in enumerate(zip_links):\n",
    "        file_url = BASE_URL + zip_file\n",
    "        local_path = os.path.join(download_folder, zip_file)\n",
    "\n",
    "        print(f\"Downloading {zip_file}...\")\n",
    "        file_response = requests.get(file_url, stream=True)\n",
    "\n",
    "        if file_response.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in file_response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        else:\n",
    "            print(\"Failed to download: {}, code {}\".format(file_url, file_response.status_code))\n",
    "            raise ValueError\n",
    "\n",
    "        # break\n",
    "\n",
    "    # 2) Extract all zip files\n",
    "    print(\"Extracting ZIP files...\")\n",
    "    for zip_file in os.listdir(download_folder):\n",
    "        if zip_file.endswith(\".zip\"):\n",
    "            with zipfile.ZipFile(os.path.join(download_folder, zip_file), 'r') as zip_ref:\n",
    "                zip_ref.extractall(extracted_folder)\n",
    "            print(f\"Extracted: {zip_file}\")\n",
    "\n",
    "    # 3) Merge all shapefiles into a GeoPackage\n",
    "    print(\"Merging shapefiles...\")\n",
    "\n",
    "    # Recursively find all shapefiles\n",
    "    shapefiles = glob.glob(os.path.join(extracted_folder, \"**\", \"*.shp\"), recursive=True)\n",
    "\n",
    "    if not shapefiles:\n",
    "        raise ValueError(\"No shapefiles found for merging.\")\n",
    "\n",
    "    # Read all shapefiles into GeoDataFrames and concatenate them\n",
    "    gdf_list = []\n",
    "    for shp in shapefiles:\n",
    "        print(f\"Reading {shp}...\")\n",
    "        gdf = gpd.read_file(shp)\n",
    "        gdf_list.append(gdf)\n",
    "\n",
    "    # Merge all GeoDataFrames\n",
    "    merged_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "\n",
    "    # save as set of CSV's\n",
    "\n",
    "    # Save to GeoPackage (overwrite if it exists)\n",
    "    merged_gdf.to_file(output_gpkg, layer=\"tracts\", driver=\"GPKG\")\n",
    "\n",
    "    print(f\"Nationwide Census Tracts saved to: {output_gpkg}\")\n",
    "\n",
    "    return merged_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_event_data(parent_dir, eventid = 'nc72282711'):\n",
    "    \"\"\"\n",
    "    Read event data from a GPKG file.\n",
    "    \"\"\"\n",
    "    event_dir = os.path.join(parent_dir, 'ShakeMaps', eventid)\n",
    "\n",
    "    # Update with the actual path\n",
    "    GPKG_PATH = os.path.join(event_dir, \"eqmodel_outputs.gpkg\")\n",
    "\n",
    "    # Read the layer you want to inspect\n",
    "    # tract_shakemap_mmi, tract_shakemap_pga, tract_shakemap_pgv --> same idea\n",
    "    gdf = gpd.read_file(GPKG_PATH, layer=\"tract_shakemap_pga\")\n",
    "    # make sure that only row that is not nan is the one we want\n",
    "    columns = gdf.columns\n",
    "    gdf = gdf[[columns[0], columns[1], columns[2], columns[3], columns[-1]]]\n",
    "    # return gdf\n",
    "    \n",
    "    # change to panda dataframe\n",
    "    pd_gdf = pd.DataFrame(gdf)\n",
    "    return pd_gdf.loc[pd_gdf[columns[1]].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(parent_dir, eventid):\n",
    "    # get event shakemaps\n",
    "    event_data = read_event_data(parent_dir, eventid)\n",
    "    states = list(event_data.GEOID.str[0:2].unique())\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>max_intensity</th>\n",
       "      <th>min_intensity</th>\n",
       "      <th>mean_intensity</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [GEOID, max_intensity, min_intensity, mean_intensity, geometry]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "read_event_data(parent_dir, 'us70008jr5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [GEOID, max_intensity, min_intensity, mean_intensity, geometry]\n",
      "Index: []\n",
      "attempting to link to https://www2.census.gov/geo/tiger/TIGER2024/TRACT/\n",
      "parsing...\n",
      "Found 0 ZIP files. Starting download...\n",
      "Download Census Data to /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/census_shp\n",
      "Extracting ZIP files...\n",
      "Extracted: tl_2024_06_tract.zip\n",
      "Merging shapefiles...\n",
      "Reading /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/extracted_census_shp/tl_2024_06_tract.shp...\n",
      "Nationwide Census Tracts saved to: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Gradschool/4_SPRING25/Capstone/github/EarthquakeDamageModel_Heinz/Data/merged_shapefile/Nationwide_Tracts.gpkg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Get the parent directory of the current working directory\n",
    "    parent_dir = os.path.dirname(os.getcwd())\n",
    "    # on google colab\n",
    "    # parent_dir = '/content/drive/MyDrive/capstone_v2'\n",
    "    event_data = read_event_data(parent_dir, eventid='us70008jr5')\n",
    "    print(event_data.head())\n",
    "    state_list = get_states(parent_dir, eventid='us70008jr5')\n",
    "    # download census of specific states\n",
    "    download_census(state_list, parent_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
